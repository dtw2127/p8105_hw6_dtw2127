---
title: "p8105_hw6_dtw2127"
author: "Dee Wang"
date: "03/12/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
```

Load birth weight data and convert variables to factors where appropriate. 

```{r}
birthweight = read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4"))
```

We'll use a combination of the knowledge base on factors associated with birth weight and a data-driven process to develop a model for birth weight. 

According to the literature, maternal race, maternal age, mother's weight, infant sex, parity, gestational weeks, and smoking all appear to impact birth weight. We would also expect baby's head circumference, baby length to be associated with birth weight. 

```{r}
library(MASS)

full.model <- lm(bwt  ~., data = birthweight)

step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
summary(step.model)

fit = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight)
```
The model generated by stepwise selection includes many of the factors that have been identified in the literature (race, infant sex, parity, gestational weeks, maternal weight, and smoking). Additionally, head circumference, length, income, and height are included in the model. 

Let's use the broom package to get a quick summary of the model and to clean up the coefficient table. 

```{r}
fit %>% 
  broom::glance()

fit %>% broom::tidy() %>% 
  dplyr::select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)
```

we'll make a plot of residuals against fitted values using add_predictions and add_residuals. 

```{r}

birthweight %>%
  add_residuals(fit) %>% 
  add_predictions(fit) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point()
```
Let's compare our model to 1) a model using length at birth and gestational age as predictors 2) a model using head circumference, length, sex and all interactions between these. 

```{r}
fit1 = lm(bwt ~ blength + gaweeks, data = birthweight)

fit2 = lm(bwt ~ bhead + blength + babysex + bhead:blength + bhead:babysex + blength:babysex + bhead:blength:babysex, data = birthweight) #check that the interaction terms were added correctly

```

We'll use cross validation to compare our model to these two other models. 

The first thing we need to do is generate training and testing datasets. We'll generate an ID column to help with creation of these datasets. We'll use 80% of the data for our training dataset and 20% for our testing dataset.

```{r}
birthweight = birthweight %>%
  mutate(ID = row_number())

train_df = sample_n(birthweight, 3474)
test_df = anti_join(birthweight, train_df, by = "ID")

```

Let's compare the distributions of the training and testing datasets using bwt and blength. 

```{r}
ggplot(train_df, aes(x = blength, y = bwt)) + 
  geom_point() + 
  geom_point(data = test_df, colour = "red")
```

The distributions are similar. 

We'll fit three models to the training data. 


```{r}
my_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = train_df)

model1 = lm(bwt ~ blength + gaweeks, data = train_df)

model2 = lm(bwt ~ bhead + blength + babysex + bhead:blength + bhead:babysex + blength:babysex + bhead:blength:babysex, data = train_df)
```

Next let's compute root mean squared errors (RMSEs).

```{r}
rmse(my_model, test_df)
rmse(model1, test_df)
rmse(model2, test_df)
```
Based off the RMSEs, it suggests that my_model works the best.

We'll iterate this process so that we can be more certain which model is best. Let's create many training and testing datasets. We'll then fit models and obtain RMSEs.

```{r}
cv_df = 
  crossv_mc(birthweight, 100) #create 100 training sets. default is 80/20 training/test split. 

cv_df = 
  cv_df %>% 
  mutate(
    my_model  = map(.x = train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x))
    ) %>% 
  mutate(
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y))
    )

cv_df = 
  cv_df %>% 
  mutate(
    model1 = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x))
    ) %>% 
  mutate(
    rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y))
    )

cv_df = 
  cv_df %>% 
  mutate(
    model2 = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + bhead:blength + bhead:babysex + blength:babysex + bhead:blength:babysex, data = .x))
    ) %>% 
  mutate(
    rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y))
    )

```

Let's plot the prediction error distribution for each of the three models. 

```{r}
cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Based on these plots, it appears that my_model is the best, followed by model2.